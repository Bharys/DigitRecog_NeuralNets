{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognition NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets,metrics\n",
    "import numpy as np\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bharath/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "\n",
    "trainx, testx, trainy, testy = ms.train_test_split(\n",
    "        digits.images.reshape((len(digits.images), -1)),\n",
    "        digits.target,\n",
    "        train_size = 0.8,\n",
    "        random_state=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1437, 64), (1437,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx.shape,trainy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=trainx\n",
    "label=trainy\n",
    "testy = testy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Digit recognition unvectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "def dsigmoid(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_digit_recog_uv(x,weight,bias):\n",
    "    input_data = x.T.copy()\n",
    "    for layer_idx,layer_num in enumerate(layer[:-1]):                  \n",
    "        data_in = np.matmul(weight[layer_idx],input_data)+bias[layer_idx]\n",
    "        data_op = sigmoid(data_in)\n",
    "        input_data = data_op.copy()    \n",
    "    return data_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Test acc: 71.94\n",
      "Epoch: 2 Test acc: 90.0\n",
      "Epoch: 3 Test acc: 92.5\n",
      "Epoch: 4 Test acc: 91.67\n",
      "Epoch: 5 Test acc: 95.0\n",
      "Epoch: 6 Test acc: 95.56\n",
      "Epoch: 7 Test acc: 96.11\n",
      "Epoch: 8 Test acc: 95.56\n",
      "Epoch: 9 Test acc: 95.83\n",
      "Epoch: 10 Test acc: 95.83\n"
     ]
    }
   ],
   "source": [
    "layer=[64,30,10]\n",
    "weight = []\n",
    "bias = []\n",
    "\n",
    "# initialize the weights & biases\n",
    "for idx,num in enumerate(layer[:-1]):\n",
    "    weight.append(np.random.normal(size=(layer[idx+1],num))/10)\n",
    "    bias.append(np.random.normal(size=(layer[idx+1],1))/10)\n",
    "\n",
    "lr = 0.055\n",
    "N = len(x)\n",
    "num_layers = len(layer)\n",
    "\n",
    "for epochs in range(10):\n",
    "    \n",
    "    for data_idx in range(N):\n",
    "        \n",
    "        # loop through each input layer\n",
    "        activation = [x[data_idx]]\n",
    "        activation_gradient = []\n",
    "        \n",
    "        #forward pass\n",
    "        for idx,layer_num in enumerate(layer[1:]):\n",
    "            input_vec = np.zeros((layer_num,1))\n",
    "            for act in range(len(activation[idx])):\n",
    "                input_vec+=(activation[idx][act]*(weight[idx][:,act])).reshape(-1,1)\n",
    "            input_vec+=bias[idx]\n",
    "            activation.append(sigmoid(input_vec).reshape(-1,1))\n",
    "            activation_gradient.append(dsigmoid(input_vec).reshape(-1,1))\n",
    "        \n",
    "        #loss\n",
    "        y = np.zeros((layer[-1],1))\n",
    "        y[label[data_idx]] = 1\n",
    "        loss = (activation[-1]-y)\n",
    "        \n",
    "        #backward pass\n",
    "        #last layer\n",
    "        delta_prev = loss*activation_gradient[-1]\n",
    "        \n",
    "        for idx,layer_num in enumerate(layer[-2::-1]):\n",
    "            \n",
    "            if(idx!=len(layer)-2):\n",
    "                # Do not compute the gradients for the input layer\n",
    "                delta_curr = np.zeros((layer_num,1))\n",
    "                for b_idx in range(len(delta_prev)):\n",
    "                    delta_curr += (delta_prev[b_idx]*weight[-1-idx][b_idx,:]).reshape(-1,1)\n",
    "                delta_curr*=activation_gradient[-2-idx]\n",
    "            \n",
    "            #SGD\n",
    "            #update weights of layer + 1\n",
    "            for w_idx in range(weight[-1-idx].shape[1]):\n",
    "                weight[-1-idx][:,w_idx]-=(lr*activation[-2-idx][w_idx]*delta_prev).flatten()\n",
    "                bias[-1-idx]-=(lr*delta_prev)\n",
    "            delta_prev = delta_curr\n",
    "    res = pred_digit_recog_uv(testx,weight,bias)\n",
    "    pred = np.argmax(res,axis=0)\n",
    "    print(\"Epoch:\",epochs+1,\"Test acc:\",np.round(sum(pred==testy)/len(testy)*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Digit recognition paritially vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Test acc: 76.39\n",
      "Epoch: 2 Test acc: 88.89\n",
      "Epoch: 3 Test acc: 91.94\n",
      "Epoch: 4 Test acc: 93.33\n",
      "Epoch: 5 Test acc: 93.61\n",
      "Epoch: 6 Test acc: 93.61\n",
      "Epoch: 7 Test acc: 94.17\n",
      "Epoch: 8 Test acc: 95.83\n",
      "Epoch: 9 Test acc: 94.17\n",
      "Epoch: 10 Test acc: 94.72\n"
     ]
    }
   ],
   "source": [
    "layer=[64,30,10]\n",
    "weight = []\n",
    "bias = []\n",
    "\n",
    "# initialize the weights & biases\n",
    "for idx,num in enumerate(layer[:-1]):\n",
    "    weight.append(np.random.normal(size=(layer[idx+1],num))/10)\n",
    "    bias.append(np.random.normal(size=(layer[idx+1],1))/10)\n",
    "\n",
    "lr = 0.05\n",
    "N = len(x)\n",
    "num_layers = len(layer)\n",
    "\n",
    "for epochs in range(10):\n",
    "    \n",
    "    for data_idx in range(N):\n",
    "        \n",
    "        # loop through each input layer\n",
    "        activation = [x[data_idx].reshape(-1,1)]\n",
    "        activation_gradient = []\n",
    "        \n",
    "        #forward pass layer-by-layer\n",
    "        for idx,layer_num in enumerate(layer[1:]):\n",
    "            input_vec = np.matmul(weight[idx],activation[idx])\n",
    "            activation.append(sigmoid(input_vec).reshape(-1,1))\n",
    "            activation_gradient.append(dsigmoid(input_vec).reshape(-1,1))\n",
    "        \n",
    "        #loss\n",
    "        y = np.zeros((layer[-1],1))\n",
    "        y[label[data_idx]] = 1\n",
    "        loss = (activation[-1]-y)\n",
    "        \n",
    "        #backward pass\n",
    "        #last layer\n",
    "        delta_prev = loss*activation_gradient[-1]\n",
    "        \n",
    "        for idx,layer_num in enumerate(layer[-2::-1]):\n",
    "            \n",
    "            if(idx!=len(layer)-2):\n",
    "                # Do not compute the gradients for the input layer\n",
    "                delta_curr = np.matmul(weight[-1-idx].T,delta_prev)\n",
    "                delta_curr*=activation_gradient[-2-idx]\n",
    "            \n",
    "            #SGD\n",
    "            #update weights of layer + 1 \n",
    "            weight[-1-idx] -= (lr*np.matmul(delta_prev,activation[-2-idx].T))\n",
    "            bias[-1-idx]-=(lr*delta_prev)\n",
    "            delta_prev = delta_curr\n",
    "    res = pred_digit_recog_uv(testx,weight,bias)\n",
    "    pred = np.argmax(res,axis=0)\n",
    "    print(\"Epoch:\",epochs+1,\"Test acc:\",np.round(sum(pred==testy)/len(testy)*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Digit recognition batch wise vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Test acc: 42.5\n",
      "Stopping training\n",
      "Desired Test accuracy 95 \n",
      "Test accuracy attained 95.0 in epoch 50\n"
     ]
    }
   ],
   "source": [
    "layer=[64,20,10]\n",
    "weight = []\n",
    "bias = []\n",
    "batch_size= 64\n",
    "desired_test_accuracy = 95\n",
    "# initialize the weights & biases with Xavier initialization\n",
    "for idx,num in enumerate(layer[:-1]):\n",
    "    weight.append(np.random.normal(size=(layer[idx+1],num))*np.sqrt(6/(layer[idx]+layer[idx+1])))\n",
    "    bias.append(np.zeros(shape=(layer[idx+1],1)))#/layer[0])\n",
    "\n",
    "lr = 1\n",
    "N = len(x)\n",
    "num_layers = len(layer)\n",
    "\n",
    "for epochs in range(1000):\n",
    "    start = 0\n",
    "    while(start<len(x)):\n",
    "        activation = [x[start:start+batch_size].T]\n",
    "        activation_gradient = []\n",
    "\n",
    "        #forward pass layer-by-layer\n",
    "        for idx,layer_num in enumerate(layer[1:]):\n",
    "            input_vec = np.matmul(weight[idx],activation[idx])\n",
    "            activation.append(sigmoid(input_vec))\n",
    "            activation_gradient.append(dsigmoid(input_vec))\n",
    "\n",
    "        #loss\n",
    "        y = np.eye(10)\n",
    "        y = y[trainy[start:start+batch_size]].T\n",
    "        loss = (activation[-1]-y)\n",
    "\n",
    "        #backward pass\n",
    "        #last layer\n",
    "        delta_prev = loss*activation_gradient[-1]\n",
    "        for idx,layer_num in enumerate(layer[-2::-1]):\n",
    "\n",
    "            if(idx!=len(layer)-2):\n",
    "                # Do not compute the gradients for the input layer\n",
    "                delta_curr = np.matmul(weight[-1-idx].T,delta_prev)\n",
    "                delta_curr*=activation_gradient[-2-idx]\n",
    "\n",
    "            #SGD\n",
    "            #update weights of layer + 1 \n",
    "            weight[-1-idx] -= (lr*np.matmul(delta_prev,activation[-2-idx].T))/batch_size\n",
    "            bias[-1-idx]-=(lr*delta_prev.sum(axis=1,keepdims=True))/batch_size\n",
    "            delta_prev = delta_curr        \n",
    "        \n",
    "        # update batch indicies\n",
    "        start+=batch_size\n",
    "    res = pred_digit_recog_uv(testx,weight,bias)\n",
    "    pred = np.argmax(res,axis=0)\n",
    "    acc = np.round(sum(pred==testy)/len(testy)*100,2)\n",
    "    if epochs%100==0:print(\"Epoch:\",epochs+1,\"Test acc:\",acc)\n",
    "    if acc >= desired_test_accuracy:\n",
    "        print(\"Stopping training\\nDesired Test accuracy {2} \\nTest accuracy attained {0} in epoch {1}\".format(acc,epochs,desired_test_accuracy))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
